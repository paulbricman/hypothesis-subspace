{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/what-are-concrete-evaluator-designs","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"What are concrete evaluator designs?\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"what-are-concrete-evaluator-designs\"\n  }, \"What are concrete evaluator designs?\"), mdx(\"p\", null, \"One design choice is whether the evaluator targets the agent's behavior directly or only the outcomes predicted to follow from given behavior. Another design choice is whether the evaluator's goal is primarily to discern human-specified behaviors/outcomes from agent ones as a classifier, or whether it's trained primarily to predict the humanness of behaviors/outcomes as a regressor (similar to \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/how-could-league-training-be-applied-to-contrastive-dreaming\",\n    \"title\": \"how-could-league-training-be-applied-to-contrastive-dreaming\"\n  }, \"[[how-could-league-training-be-applied-to-contrastive-dreaming]]\"), \").\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"behavior-directed regressor\"), \": The evaluator takes in behaviors (i.e. human-specified or artificial), and outputs two values: the humanness and the behavior \\\"age\\\". If the input behavior was generated by an epoch-seven agent, the humanness should be low and the age should be close to seven. If the input was generated by an epoch-seven \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" procedure, it should be the same. If the input was human-specified, however, the humanness and age should both be high.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"outcome-directed regressor\"), \": Just like the previous one, except the inputs are not behaviors, but predicted states of the world which would follow from the unknown behaviors. Same designated outputs apply.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"behavior-directed classifier\"), \": Just like the analogous regressor, the evaluator takes in behaviors, but predicts one class among: human-specified, suggested by agent one, suggested by \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" of its first version, suggested by agent two, etc.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"outcome-directed classifier\"), \": The outputs are similar to the other classifier, but the inputs are outcomes, rather than behaviors, just like in the other outcome-directed design.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"behavior-directed mix\"), \": Same as other behavior-targeted ones, but the output is one class among a fixed set of classes (i.e. human-specified, agent, adversarial), and one numerical value for \\\"age.\\\"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"outcome-directed mix\"), \": Just like the other mix for outputs, but inputs are outcomes instead of behaviors.\")));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"How could league training be applied to contrastive dreaming?\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"how-could-league-training-be-applied-to-contrastive-dreaming\"\n  }, \"How could league training be applied to contrastive dreaming?\"), mdx(\"p\", null, \"Actually, contrastive dreaming (CD), league training (LT), and adversarial training regimes (GAN) might all three work together really neatly. I see three broad major setups:\"), mdx(\"h2\", {\n    \"id\": \"cd--lt\"\n  }, \"CD + LT\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train evaluator E1 to penalize its contrastive dreams D1 while correlating with human ratings.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train evaluator E2 to penalize D1 and D2 while correlating with human ratings.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"...\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"E \\u2192 figure its systematic blindspots.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A \\u2192 N/A\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"E > A\")), mdx(\"h2\", {\n    \"id\": \"gan--lt\"\n  }, \"GAN + LT\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train evaluator to discriminate among human plans / the agent A1's artificial plans.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train evaluator to discriminate among human plans / A1's plans / A2's plans.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"...\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"E \\u2192 figure agent's systematic tricks.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A \\u2192 figure evaluator's systematic blindspots.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"E = A\")), mdx(\"h2\", {\n    \"id\": \"cd--gan--lt\"\n  }, \"CD + GAN + LT\"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train evaluator E1 to discriminate among human plans / A1's plans / its contrastive dreams D1.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Train evaluator E2 to discriminate among human plans / A1's plans / D1 / A2's plans, D2.\"), mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"...\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"E \\u2192 figure agent's systematic tricks and its own blindspots.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"A \\u2192 figure evaluator's systematic blindspots.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"E > A\")), mdx(\"p\", null, \"I have shuffled plans as evaluation targets with world states somewhat. World states most intuitively go with regression targets, while plans go with classification ones. However, that's not mandatory. You can assign humanness to plans to frame it as a regression problem, or classify world states as appropriate/inappropriate, for instance.\"));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"aliases":[]},"parent":{"__typename":"File","id":"2782b6ae-5c75-5590-913f-9385b18c9008","fields":{"slug":"/how-could-league-training-be-applied-to-contrastive-dreaming","title":"How could league training be applied to contrastive dreaming?"}}},{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Contrastive Dreaming\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"contrastive-dreaming\"\n  }, \"Contrastive Dreaming\"), mdx(\"p\", null, \"Dreaming has been argued to act as a source of negative examples (i.e. how the world isn't like), in order to complement the positive examples of wakefulness. In \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"https://en.wikipedia.org/wiki/DeepDream\"\n  }, \"DeepDream art\"), \", people force AIs to project internal models of their world onto their world (e.g. by mutating input images into extreme dogginess). While those hallucinations generally point in the right direction, they always violate reality (e.g. ultra-doggified images fail to depict how dogs really show up in the world \\u2014 you can easily tell that the image is DeepDreamed). That makes for a perfect source of negative examples to complement robust adversarial training, because dreamed up data is simultaneously not how the world is and how the model thinks the world is. This contrastive dreaming scheme might improve generalization in evaluators meant to operationalize human ideals of the world, and provide a dense source of edge cases to be prioritized in (human) oversight.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/how-does-contrastive-dreaming-relate-to-concrete-challenges-in-alignment\",\n    \"title\": \"how-does-contrastive-dreaming-relate-to-concrete-challenges-in-alignment\"\n  }, \"[[how-does-contrastive-dreaming-relate-to-concrete-challenges-in-alignment]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-are-some-other-applications-of-contrastive-dreaming-besides-objective-robustness\",\n    \"title\": \"what-are-some-other-applications-of-contrastive-dreaming-besides-objective-robustness\"\n  }, \"[[what-are-some-other-applications-of-contrastive-dreaming-besides-objective-robustness]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/how-could-league-training-be-applied-to-contrastive-dreaming\",\n    \"title\": \"how-could-league-training-be-applied-to-contrastive-dreaming\"\n  }, \"[[how-could-league-training-be-applied-to-contrastive-dreaming]]\"), \"\")));\n}\n;\nMDXContent.isMDXComponent = true;","frontmatter":{"aliases":[]},"parent":{"__typename":"File","id":"2c829204-3d93-5141-a958-cf58d1cc1959","fields":{"slug":"/contrastive-dreaming","title":"Contrastive Dreaming"}}}],"inboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"What if the evaluator overfits to the seed?\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"what-if-the-evaluator-overfits-to-the-seed\"\n  }, \"What if the evaluator overfits to the seed?\"), mdx(\"p\", null, \"It might be plausible that the evaluator learns to perfectly spot the behaviors or outcomes specified as desirable by humans, and simply picks out anything else as inappropriate. In this situation, the agent would similarly be incentivized to closely mimic human input verbatim. This appears problematic, especially considering the fact that an agent's affordances might drastically expand as it gains in capability.Perhaps behavior-driven evaluators might therefore be dead-ends when considering \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/what-are-concrete-evaluator-designs\",\n    \"title\": \"what-are-concrete-evaluator-designs\"\n  }, \"[[what-are-concrete-evaluator-designs]]\"), \".\"), mdx(\"p\", null, \"Would the same apply for outcome-directed evaluators? The evaluator might only accept the human-specified verbatim outcomes as appropriate, which might be limiting in some sense considering available capability. Due to the dataset size of human images, this issue doesn't show up that much in image synthesis using GANs compared to other issues like mode collapse.\"));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"18a4cf51-225a-5997-b2a6-3e0d13476381","fields":{"slug":"/what-if-evaluator-overfits-to-seed","title":"What if the evaluator overfits to the seed?"}}},{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Oversight Leagues\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"oversight-leagues\"\n  }, \"Oversight Leagues\"), mdx(\"p\", null, \"Similar to how \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/ideological-inference-engines\",\n    \"title\": \"ideological-inference-engines\"\n  }, \"[[ideological-inference-engines]]\"), \" are a merger of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/deontic-arrays\",\n    \"title\": \"deontic-arrays\"\n  }, \"[[deontic-arrays]]\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/memetic-colonies\",\n    \"title\": \"memetic-colonies\"\n  }, \"[[memetic-colonies]]\"), \", oversight leagues are a merger of ideas from \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/parametric-ecologies\",\n    \"title\": \"parametric-ecologies\"\n  }, \"[[parametric-ecologies]]\"), \", an attempt to combine the strength of the previous frames. Oversight leagues rely on placing the agent-in-training and an overseer evaluator in a GAN-like co-evolving feedback loop (a training regime expressible in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/parametric-ecologies\",\n    \"title\": \"parametric-ecologies\"\n  }, \"[[parametric-ecologies]]\"), \"), while keeping the evaluator one step ahead using robustness techniqies (of which \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" is an example). Additionally, notions from league training are brought in to complete the frame. In general, an oversight league consists of:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"unilateral training signals\"), \": Ways of improving the performance of one single component of the agent-evaluator duo without making use of the other. For instance, \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" or vanilla adversarial training. An evaluator might be trained to systematically predict its own blindspots.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"bilateral training signals\"), \": Ways of improving the performance of a component of the agent-evaluator duo using the other component. An evaluator might be trained to spot the agent's shortcomings, while an agent might be trained to pick up the evaluator's blindspots. Bilateral training signals further split into:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"present exploitation\"), \": Training a component against the latest version of the opposite component.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"past exploitation\"), \": Training a component against all the past versions of the opposite component, or a tricky mixture thereof. This is meant to avoid a component forgetting to account for earlier exploitation strategies (based on \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.nature.com/articles/s41586-019-1724-z\"\n  }, \"league exploiters in AlphaStar\"), \").\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"future exploitation\"), \": Train a component against future versions of the opposite component by anticipating short-term counterplay. Consists in running optimization across an unrolled version of a (boxed!) version of the opponent which gets a few more optimization steps (based on \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/pdf/1611.02163.pdf\"\n  }, \"unrolled GANs\"), \").\")))), mdx(\"p\", null, \"The core idea behind oversight leagues is that the evaluator is helped to better understand its systematic blindspot in relation to the agent. The agent is helped in a similar way, but unilateral training signals can be used to keep the overseer one step ahead.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-are-concrete-evaluator-designs\",\n    \"title\": \"what-are-concrete-evaluator-designs\"\n  }, \"[[what-are-concrete-evaluator-designs]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-are-concrete-unilateral-training-signals\",\n    \"title\": \"what-are-concrete-unilateral-training-signals\"\n  }, \"[[what-are-concrete-unilateral-training-signals]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-the-agent-gets-one-step-ahead-by-chance\",\n    \"title\": \"what-if-the-agent-gets-one-step-ahead-by-chance\"\n  }, \"[[what-if-the-agent-gets-one-step-ahead-by-chance]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-evaluator-overfits-to-seed\",\n    \"title\": \"what-if-evaluator-overfits-to-seed\"\n  }, \"[[what-if-evaluator-overfits-to-seed]]\"), \"\")));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"c971cb02-84ce-519e-9675-1ed9fb428398","fields":{"slug":"/oversight-leagues","title":"Oversight Leagues"}}}]},"fields":{"slug":"/what-are-concrete-evaluator-designs","title":"What are concrete evaluator designs?"}}},"pageContext":{"id":"ac1be746-1fe1-5417-a5e2-6e3a5907b118"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}