{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/what-if-the-agent-gets-one-step-ahead-by-chance","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"What if the agent gets one step ahead by chance?\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"what-if-the-agent-gets-one-step-ahead-by-chance\"\n  }, \"What if the agent gets one step ahead by chance?\"), mdx(\"p\", null, \"What if, by whatever means, the agent somehow gets one step ahead of the evaluator. It's capable of exploiting its blindspots better than it in turn can spot its tricks. In this position, the agent can plausibly maneuver itself in a rewarding-yet-misaligned position by subtly remaining under the evaluator's radar. If there would always a small chance for this to happen, then it would be virtually guaranteed to happen over a long period of time, which is unfortunate.\"), mdx(\"p\", null, \"The best way of avoiding nasty outcomes of this state would be not to get into this state in the first place. This is where unilateral training signals to beef up the evaluator would come into play, combined perhaps with future exploitation giving the evaluator a step ahead in anticipating the agent's moves and reacting to it. Provable robustness in a future exploitation setting might make for a robust foundationof this approach.\"), mdx(\"p\", null, \"However, assuming the agent still gets one step ahead, what could be done? It might lead to a spike in reward. Maybe spikes in reward should be penalized? This would make the whole training process slower, especially in the beginning. Maybe allow early reward spikes but don't allow that later on?\"));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[],"inboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Oversight Leagues\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"oversight-leagues\"\n  }, \"Oversight Leagues\"), mdx(\"p\", null, \"Similar to how \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/ideological-inference-engines\",\n    \"title\": \"ideological-inference-engines\"\n  }, \"[[ideological-inference-engines]]\"), \" are a merger of \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/deontic-arrays\",\n    \"title\": \"deontic-arrays\"\n  }, \"[[deontic-arrays]]\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/memetic-colonies\",\n    \"title\": \"memetic-colonies\"\n  }, \"[[memetic-colonies]]\"), \", oversight leagues are a merger of ideas from \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" and \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/parametric-ecologies\",\n    \"title\": \"parametric-ecologies\"\n  }, \"[[parametric-ecologies]]\"), \", an attempt to combine the strength of the previous frames. Oversight leagues rely on placing the agent-in-training and an overseer evaluator in a GAN-like co-evolving feedback loop (a training regime expressible in \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/parametric-ecologies\",\n    \"title\": \"parametric-ecologies\"\n  }, \"[[parametric-ecologies]]\"), \"), while keeping the evaluator one step ahead using robustness techniqies (of which \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" is an example). Additionally, notions from league training are brought in to complete the frame. In general, an oversight league consists of:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"unilateral training signals\"), \": Ways of improving the performance of one single component of the agent-evaluator duo without making use of the other. For instance, \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/contrastive-dreaming\",\n    \"title\": \"contrastive-dreaming\"\n  }, \"[[contrastive-dreaming]]\"), \" or vanilla adversarial training. An evaluator might be trained to systematically predict its own blindspots.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"bilateral training signals\"), \": Ways of improving the performance of a component of the agent-evaluator duo using the other component. An evaluator might be trained to spot the agent's shortcomings, while an agent might be trained to pick up the evaluator's blindspots. Bilateral training signals further split into:\", mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"present exploitation\"), \": Training a component against the latest version of the opposite component.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"past exploitation\"), \": Training a component against all the past versions of the opposite component, or a tricky mixture thereof. This is meant to avoid a component forgetting to account for earlier exploitation strategies (based on \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://www.nature.com/articles/s41586-019-1724-z\"\n  }, \"league exploiters in AlphaStar\"), \").\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"future exploitation\"), \": Train a component against future versions of the opposite component by anticipating short-term counterplay. Consists in running optimization across an unrolled version of a (boxed!) version of the opponent which gets a few more optimization steps (based on \", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"https://arxiv.org/pdf/1611.02163.pdf\"\n  }, \"unrolled GANs\"), \").\")))), mdx(\"p\", null, \"The core idea behind oversight leagues is that the evaluator is helped to better understand its systematic blindspot in relation to the agent. The agent is helped in a similar way, but unilateral training signals can be used to keep the overseer one step ahead.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-are-concrete-evaluator-designs\",\n    \"title\": \"what-are-concrete-evaluator-designs\"\n  }, \"[[what-are-concrete-evaluator-designs]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-are-concrete-unilateral-training-signals\",\n    \"title\": \"what-are-concrete-unilateral-training-signals\"\n  }, \"[[what-are-concrete-unilateral-training-signals]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-the-agent-gets-one-step-ahead-by-chance\",\n    \"title\": \"what-if-the-agent-gets-one-step-ahead-by-chance\"\n  }, \"[[what-if-the-agent-gets-one-step-ahead-by-chance]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-evaluator-overfits-to-seed\",\n    \"title\": \"what-if-evaluator-overfits-to-seed\"\n  }, \"[[what-if-evaluator-overfits-to-seed]]\"), \"\")));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"c971cb02-84ce-519e-9675-1ed9fb428398","fields":{"slug":"/oversight-leagues","title":"Oversight Leagues"}}}]},"fields":{"slug":"/what-if-the-agent-gets-one-step-ahead-by-chance","title":"What if the agent gets one step ahead by chance?"}}},"pageContext":{"id":"9fd039b9-ec90-549c-a5af-a31c3016660c"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}