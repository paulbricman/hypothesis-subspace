{"componentChunkName":"component---node-modules-gatsby-theme-garden-src-templates-local-file-js","path":"/what-if-the-two-component-rewards-are-unstable","result":{"data":{"file":{"childMdx":{"body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"What if the two component rewards are unstable?\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"what-if-the-two-component-rewards-are-unstable\"\n  }, \"What if the two component rewards are unstable?\"), mdx(\"p\", null, \"In its vanilla formulation, a deontic array is supposed to contribute part of the aggregate reward given to Alex, together with the main original reward associated with human feedback. Let's refer to those as human feedback (HF) reward, deontic reward, and aggregate reward (composed of the previous two). In this situation, what if Alex expects excellent human feedback from a course of action with poor deontic reward? What if HF reward trumps the deontic one, with large potential payoffs steering Alex away from morality?\"), mdx(\"p\", null, \"To tackle this situations, it might help to make either make both HF and deontic reward bounded or unbounded. If HF is unbounded while deontic is not, there's no question about what will end up driving Alex more. In the equality cases, they might be more comparable, with both bounded appearing safer.\"), mdx(\"p\", null, \"Besides individual bounds on reward sources, the aggregation scheme involved in computing the aggregate reward could penalize large differences between HF and deontic. If HF is huge but deontic is low, the aggregate could be much closer to deontic via e.g. a geometric mean. However, humans might themselves want to occasionally clash with the charter for various reasons. Such an aggregation scheme would naturally also limit their impact.\"));\n}\n;\nMDXContent.isMDXComponent = true;","outboundReferences":[],"inboundReferences":[{"__typename":"Mdx","body":"var _excluded = [\"components\"];\n\nfunction _extends() { _extends = Object.assign ? Object.assign.bind() : function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsxRuntime classic */\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"How could deontic arrays help avoid HFDT takeover?\"\n};\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, _excluded);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"h1\", {\n    \"id\": \"how-could-deontic-arrays-help-avoid-hfdt-takeover\"\n  }, \"How could deontic arrays help avoid HFDT takeover?\"), mdx(\"p\", null, \"Deontic arrays would be applied to the model being trained in the \", mdx(\"a\", {\n    parentName: \"p\",\n    \"href\": \"/hfdt-takeover-scenario\",\n    \"title\": \"hfdt-takeover-scenario\"\n  }, \"[[hfdt-takeover-scenario]]\"), \" as an additional component of the optimization target. The approach would be applicable to all training regimes present in the scenario: self-supervised, supervised, and reinforcement learning.\"), mdx(\"p\", null, \"In its basic form, the technique would be employed as follows. First, a large charter of normative principles expressed in written language would be collected from various sources. Those should contain huge amounts of redundancy (i.e. expressing the same principle in various formulations). The charter does not have to be internally consistent -- principles are allowed to occasionally clash.\"), mdx(\"p\", null, \"Second, the charter would (automatically) be converted into an anti-charter which contains a negated version of each principle mentioned in the original charter.\"), mdx(\"p\", null, \"Third, the two charters would be treated as collections of token sequences (i.e. sequences of words/subwords/characters). Given those two sequence sets, the model being trained would be incentived to output action sequences from which \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"the original principles are more likely to follow compared to the negated versions\"), \". This requires a multi-modal model (e.g. the same model, a separate one, a past copy, etc.) to turn the action history and principles into a numerical reward signal. This signal would then join the main reward to form a final objective through an aggregation scheme (e.g. linear combination, geometric mean, etc.).\"), mdx(\"p\", null, \"Fourth, the charter (and anti-charter) would be extended in parallel to overspecify the desired normative framework in an attempt to e.g. avoid quibbling over the specific letter of the law. The vanilla version of deontic arrays relies on what could be called \\\"counterfactual cross-validation,\\\" which goes as follows. If in a completely sandboxed environment with only one output bit we temporarily discard part of the charter, is the model prevented from violating those anyways thanks to the remaining principles redundantly making up for them? If not, target external red teaming efforts to automatically patch up that brittle part of the normative framework. Potentially discount older principles to allow some drift, in a style loosely related to CEV by memetic colonies.\"), mdx(\"p\", null, \"As a bonus, deontic arrays might help instill an aversion towards taking decisions which might then in turn lead to charter-violating actions later on, especially in a reinforcement learning regime. The model might grow to value actions which don't place it in morally ambiguous situations in the future, giving itself less opportunity to err.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-the-charter-grows-unwieldy-in-size\",\n    \"title\": \"what-if-the-charter-grows-unwieldy-in-size\"\n  }, \"[[what-if-the-charter-grows-unwieldy-in-size]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-the-seed-charter-misses-critical-parts-of-the-implicit-normative-framework\",\n    \"title\": \"what-if-the-seed-charter-misses-critical-parts-of-the-implicit-normative-framework\"\n  }, \"[[what-if-the-seed-charter-misses-critical-parts-of-the-implicit-normative-framework]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-alex-games-the-expanding-charter\",\n    \"title\": \"what-if-alex-games-the-expanding-charter\"\n  }, \"[[what-if-alex-games-the-expanding-charter]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-alex-hacks-the-deontic-array\",\n    \"title\": \"what-if-alex-hacks-the-deontic-array\"\n  }, \"[[what-if-alex-hacks-the-deontic-array]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-moral-absolutism-is-misguided\",\n    \"title\": \"what-if-moral-absolutism-is-misguided\"\n  }, \"[[what-if-moral-absolutism-is-misguided]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-immoral-means-are-required-for-moral-ends\",\n    \"title\": \"what-if-immoral-means-are-required-for-moral-ends\"\n  }, \"[[what-if-immoral-means-are-required-for-moral-ends]]\"), \"\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"\", mdx(\"a\", {\n    parentName: \"li\",\n    \"href\": \"/what-if-the-two-component-rewards-are-unstable\",\n    \"title\": \"what-if-the-two-component-rewards-are-unstable\"\n  }, \"[[what-if-the-two-component-rewards-are-unstable]]\"), \"\")));\n}\n;\nMDXContent.isMDXComponent = true;","parent":{"__typename":"File","id":"d00481bd-f8cb-5990-b827-e5f36400946a","fields":{"slug":"/how-could-deontic-arrays-help-avoid-hfdt-takeover","title":"How could deontic arrays help avoid HFDT takeover?"}}}]},"fields":{"slug":"/what-if-the-two-component-rewards-are-unstable","title":"What if the two component rewards are unstable?"}}},"pageContext":{"id":"e77d7409-9fe6-57a6-a8df-e167a82468bc"}},"staticQueryHashes":["2098632890","2221750479","2468095761"]}